{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this notebook is to present the basics of Spark: why we use it, and what kind of operations we can perform using it.\n",
    "\n",
    "In this notebook we cover the following topics:\n",
    "- SparkContext object\n",
    "- RDDs\n",
    "- Operations on RDDs:\n",
    "    - transformations\n",
    "    - actions\n",
    "- DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext\n",
    "\n",
    "First of all, we need to create a driver Spark application which communicates the user commands to the Spark Workers. We do so with the help of the `SparkContext` object, which coordinates with the cluster manager about the resources required for execution and the actual tasks that need to be executed. The resources required can be defined within the `SparkConf` configuration object, which we pass as a parameter to the `SparkContext` object upon creation.\n",
    "\n",
    "Once we create a `SparkContext (sc)` object, we use it for orchestrating the allocated resources. The process is illustrated on the image below. Each of the components on the image run in a different process, enabling parallel execution of the worker nodes. This setup is replicated for all applications submitted to the cluster, and due to the process isolation, they cannot communicate directly (exchange data) among each other without persisting it to the disk.\n",
    "\n",
    "For more information on how Spark applications are deployed, please refer to the following resources:\n",
    "- [Submitting Spark Applications](https://spark.apache.org/docs/latest/submitting-applications.html)\n",
    "- [Cluster Mode Overview](https://spark.apache.org/docs/latest/submitting-applications.html)\n",
    "\n",
    "![SparkContext overview](data/cluster_overview.png)\n",
    "\n",
    "Image source: https://spark.apache.org/docs/latest/cluster-overview.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a SparkConf configuration object which sets the Spark Application name\n",
    "conf = SparkConf().setAppName(\"Spark Intro\")\n",
    "\n",
    "# Create the SparkContext object.\n",
    "# In this case we are using `.getOrCreate` method to be able to rerun the same cell multiple times\n",
    "sc = SparkContext.getOrCreate(conf=conf) # Alternatively, use SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets - RDDs\n",
    "\n",
    "Resilient Distributed Dataset (RDD) is the elementary data structure used in Spark. RDDs are immutable and fault-tolerant, meaning that once loaded, the data cannot be changed, and because of that the system is able to recalculate results from failing nodes. RDDs also enable operations on the enclosed data to be executed in parallel on multiple nodes.\n",
    "\n",
    "There are 3 main ways of creating RDDs:\n",
    "1. From an existing collection - parallelize the existing collection from the target programming language, such as an array.\n",
    "2. Transforming an existing RDD - applying transformations on an RDD yields a new RDD.\n",
    "3. Loading an external dataset - ability to load data from an existing source (e.g. from a file system)\n",
    "\n",
    "There are 2 types of operations that can be applied to RDDs:\n",
    "1. Transformations - a lazy operation which yields a new RDD\n",
    "2. Actions - return a value to the driver program, i.e. execute all the predefined lazy operations of the RDD\n",
    "\n",
    "We are going to explore all of these options in the examples below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, stop the application.\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
